{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting scrappy\n",
      "  Using cached https://files.pythonhosted.org/packages/b4/df/dcd763f44aea90fbd58b4e7f7c8be95ade7d130cd21251fc0d93c295b1f0/Scrappy-0.3.0.alpha.4.tar.gz\n",
      "Collecting guessit (from scrappy)\n",
      "  Using cached https://files.pythonhosted.org/packages/09/01/523ec092ed900893100bec16c24674aff0d0b0af5a7f85654069f5cedb7e/guessit-3.4.3-py3-none-any.whl\n",
      "Collecting tvdb_api (from scrappy)\n",
      "  Using cached https://files.pythonhosted.org/packages/a9/66/7f9c6737be8524815a02dd2edd3a24718fa786614573104342eae8d2d08b/tvdb_api-3.1.0.tar.gz\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "    Preparing wheel metadata: started\n",
      "    Preparing wheel metadata: finished with status 'done'\n",
      "Collecting hachoir-metadata (from scrappy)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  ERROR: Could not find a version that satisfies the requirement hachoir-metadata (from scrappy) (from versions: none)\n",
      "ERROR: No matching distribution found for hachoir-metadata (from scrappy)\n"
     ]
    }
   ],
   "source": [
    "!pip install scrappy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting scrapy\n",
      "  Downloading https://files.pythonhosted.org/packages/47/9e/011e2ed50af67373067c518510eac1c07c5a1a9d54a1064dd08c6518d0c4/Scrapy-2.6.1-py2.py3-none-any.whl (264kB)\n",
      "Collecting parsel>=1.5.0 (from scrapy)\n",
      "  Downloading https://files.pythonhosted.org/packages/23/1e/9b39d64cbab79d4362cdd7be7f5e9623d45c4a53b3f7522cd8210df52d8e/parsel-1.6.0-py2.py3-none-any.whl\n",
      "Collecting zope.interface>=4.1.3 (from scrapy)\n",
      "  Downloading https://files.pythonhosted.org/packages/35/89/5fb686f3cd67b4b8193741f8c47ffc871c089d28eb5735c1c1871de13fcb/zope.interface-5.4.0-cp37-cp37m-win_amd64.whl (210kB)\n",
      "Requirement already satisfied: pyOpenSSL>=16.2.0 in c:\\users\\rajeev ray\\anaconda3\\lib\\site-packages (from scrapy) (19.0.0)\n",
      "Collecting service-identity>=16.0.0 (from scrapy)\n",
      "  Downloading https://files.pythonhosted.org/packages/93/5a/5e93f280ec7be676b5a57f305350f439d31ced168bca04e6ffa64b575664/service_identity-21.1.0-py2.py3-none-any.whl\n",
      "Requirement already satisfied: w3lib>=1.17.0 in c:\\users\\rajeev ray\\anaconda3\\lib\\site-packages (from scrapy) (1.22.0)\n",
      "Collecting queuelib>=1.4.2 (from scrapy)\n",
      "  Downloading https://files.pythonhosted.org/packages/06/1e/9e3bfb6a10253f5d95acfed9c5732f4abc2ef87bdf985594ddfb99d222da/queuelib-1.6.2-py2.py3-none-any.whl\n",
      "Collecting Twisted>=17.9.0 (from scrapy)\n",
      "  Downloading https://files.pythonhosted.org/packages/db/99/38622ff95bb740bcc991f548eb46295bba62fcb6e907db1987c4d92edd09/Twisted-22.4.0-py3-none-any.whl (3.1MB)\n",
      "Collecting protego>=0.1.15 (from scrapy)\n",
      "  Downloading https://files.pythonhosted.org/packages/81/4d/3e01f10d6dd2d35793711c2e27a07e547c6aec0ab8d3199bb83e68956fdb/Protego-0.2.1-py2.py3-none-any.whl\n",
      "Collecting tldextract (from scrapy)\n",
      "  Downloading https://files.pythonhosted.org/packages/8f/ef/6a05da5e708016b495b2d559c773c7f89fc87fd683058697a89e237e30f3/tldextract-3.3.0-py3-none-any.whl (93kB)\n",
      "Collecting itemadapter>=0.1.0 (from scrapy)\n",
      "  Downloading https://files.pythonhosted.org/packages/ca/75/efdbf278949d654969e931a7690e20092aa482980e01d81e7edf2fe2e55e/itemadapter-0.6.0-py3-none-any.whl\n",
      "Requirement already satisfied: lxml>=3.5.0; platform_python_implementation == \"CPython\" in c:\\users\\rajeev ray\\anaconda3\\lib\\site-packages (from scrapy) (4.4.1)\n",
      "Requirement already satisfied: cssselect>=0.9.1 in c:\\users\\rajeev ray\\anaconda3\\lib\\site-packages (from scrapy) (1.1.0)\n",
      "Collecting PyDispatcher>=2.0.5; platform_python_implementation == \"CPython\" (from scrapy)\n",
      "  Downloading https://files.pythonhosted.org/packages/cd/37/39aca520918ce1935bea9c356bcbb7ed7e52ad4e31bff9b943dfc8e7115b/PyDispatcher-2.0.5.tar.gz\n",
      "Collecting itemloaders>=1.0.1 (from scrapy)\n",
      "  Downloading https://files.pythonhosted.org/packages/b3/2b/eb2ddf7becf834679273a6f79ffdc6fbedf07c5272e2eddf412582143c0e/itemloaders-1.0.4-py3-none-any.whl\n",
      "Requirement already satisfied: setuptools in c:\\users\\rajeev ray\\anaconda3\\lib\\site-packages (from scrapy) (41.4.0)\n",
      "Requirement already satisfied: cryptography>=2.0 in c:\\users\\rajeev ray\\anaconda3\\lib\\site-packages (from scrapy) (2.7)\n",
      "Requirement already satisfied: six>=1.6.0 in c:\\users\\rajeev ray\\anaconda3\\lib\\site-packages (from parsel>=1.5.0->scrapy) (1.12.0)\n",
      "Requirement already satisfied: attrs>=19.1.0 in c:\\users\\rajeev ray\\anaconda3\\lib\\site-packages (from service-identity>=16.0.0->scrapy) (19.2.0)\n",
      "Collecting pyasn1-modules (from service-identity>=16.0.0->scrapy)\n",
      "  Downloading https://files.pythonhosted.org/packages/95/de/214830a981892a3e286c3794f41ae67a4495df1108c3da8a9f62159b9a9d/pyasn1_modules-0.2.8-py2.py3-none-any.whl (155kB)\n",
      "Collecting pyasn1 (from service-identity>=16.0.0->scrapy)\n",
      "  Downloading https://files.pythonhosted.org/packages/62/1e/a94a8d635fa3ce4cfc7f506003548d0a2447ae76fd5ca53932970fe3053f/pyasn1-0.4.8-py2.py3-none-any.whl (77kB)\n",
      "Requirement already satisfied: typing-extensions>=3.6.5 in c:\\users\\rajeev ray\\anaconda3\\lib\\site-packages (from Twisted>=17.9.0->scrapy) (4.2.0)\n",
      "Collecting twisted-iocpsupport<2,>=1.0.2; platform_system == \"Windows\" (from Twisted>=17.9.0->scrapy)\n",
      "  Downloading https://files.pythonhosted.org/packages/2d/29/1c4b919f715bd741b95b10e077c359da150302dd1243493ac95001c876d4/twisted_iocpsupport-1.0.2-cp37-cp37m-win_amd64.whl (44kB)\n",
      "Collecting Automat>=0.8.0 (from Twisted>=17.9.0->scrapy)\n",
      "  Downloading https://files.pythonhosted.org/packages/dd/83/5f6f3c1a562674d65efc320257bdc0873ec53147835aeef7762fe7585273/Automat-20.2.0-py2.py3-none-any.whl\n",
      "Collecting hyperlink>=17.1.1 (from Twisted>=17.9.0->scrapy)\n",
      "  Downloading https://files.pythonhosted.org/packages/6e/aa/8caf6a0a3e62863cbb9dab27135660acba46903b703e224f14f447e57934/hyperlink-21.0.0-py2.py3-none-any.whl (74kB)\n",
      "Collecting incremental>=21.3.0 (from Twisted>=17.9.0->scrapy)\n",
      "  Downloading https://files.pythonhosted.org/packages/99/3b/4f80dd10cb716f3a9e22ae88f026d25c47cc3fdf82c2747f3d59c98e4ff1/incremental-21.3.0-py2.py3-none-any.whl\n",
      "Collecting constantly>=15.1 (from Twisted>=17.9.0->scrapy)\n",
      "  Downloading https://files.pythonhosted.org/packages/b9/65/48c1909d0c0aeae6c10213340ce682db01b48ea900a7d9fce7a7910ff318/constantly-15.1.0-py2.py3-none-any.whl\n",
      "Collecting requests-file>=1.4 (from tldextract->scrapy)\n",
      "  Downloading https://files.pythonhosted.org/packages/77/86/cdb5e8eaed90796aa83a6d9f75cfbd37af553c47a291cd47bc410ef9bdb2/requests_file-1.5.1-py2.py3-none-any.whl\n",
      "Requirement already satisfied: filelock>=3.0.8 in c:\\users\\rajeev ray\\anaconda3\\lib\\site-packages (from tldextract->scrapy) (3.0.12)\n",
      "Requirement already satisfied: idna in c:\\users\\rajeev ray\\anaconda3\\lib\\site-packages (from tldextract->scrapy) (2.8)\n",
      "Requirement already satisfied: requests>=2.1.0 in c:\\users\\rajeev ray\\anaconda3\\lib\\site-packages (from tldextract->scrapy) (2.22.0)\n",
      "Collecting jmespath>=0.9.5 (from itemloaders>=1.0.1->scrapy)\n",
      "  Downloading https://files.pythonhosted.org/packages/ca/be/48152dbe2e16b1960fb7987639b2426a0609245041d63a2fa96d1ef88da3/jmespath-1.0.0-py3-none-any.whl\n",
      "Requirement already satisfied: asn1crypto>=0.21.0 in c:\\users\\rajeev ray\\anaconda3\\lib\\site-packages (from cryptography>=2.0->scrapy) (1.0.1)\n",
      "Requirement already satisfied: cffi!=1.11.3,>=1.8 in c:\\users\\rajeev ray\\anaconda3\\lib\\site-packages (from cryptography>=2.0->scrapy) (1.15.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\rajeev ray\\anaconda3\\lib\\site-packages (from requests>=2.1.0->tldextract->scrapy) (2022.5.18.1)\n",
      "Collecting urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 (from requests>=2.1.0->tldextract->scrapy)\n",
      "  Using cached https://files.pythonhosted.org/packages/56/aa/4ef5aa67a9a62505db124a5cb5262332d1d4153462eb8fd89c9fa41e5d92/urllib3-1.25.11-py2.py3-none-any.whl\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in c:\\users\\rajeev ray\\anaconda3\\lib\\site-packages (from requests>=2.1.0->tldextract->scrapy) (3.0.4)\n",
      "Requirement already satisfied: pycparser in c:\\users\\rajeev ray\\anaconda3\\lib\\site-packages (from cffi!=1.11.3,>=1.8->cryptography>=2.0->scrapy) (2.19)\n",
      "Building wheels for collected packages: PyDispatcher\n",
      "  Building wheel for PyDispatcher (setup.py): started\n",
      "  Building wheel for PyDispatcher (setup.py): finished with status 'done'\n",
      "  Created wheel for PyDispatcher: filename=PyDispatcher-2.0.5-cp37-none-any.whl size=12551 sha256=1422071d7845827f1d3828a18ff3d2017ef2af6f41d8f74344773a055f5d7988\n",
      "  Stored in directory: C:\\Users\\Rajeev Ray\\AppData\\Local\\pip\\Cache\\wheels\\88\\99\\96\\cfef6665f9cb1522ee6757ae5955feedf2fe25f1737f91fa7f\n",
      "Successfully built PyDispatcher\n",
      "Installing collected packages: parsel, zope.interface, pyasn1, pyasn1-modules, service-identity, queuelib, twisted-iocpsupport, Automat, hyperlink, incremental, constantly, Twisted, protego, requests-file, tldextract, itemadapter, PyDispatcher, jmespath, itemloaders, scrapy, urllib3\n",
      "  Found existing installation: urllib3 1.26.9\n",
      "    Uninstalling urllib3-1.26.9:\n",
      "      Successfully uninstalled urllib3-1.26.9\n",
      "Successfully installed Automat-20.2.0 PyDispatcher-2.0.5 Twisted-22.4.0 constantly-15.1.0 hyperlink-21.0.0 incremental-21.3.0 itemadapter-0.6.0 itemloaders-1.0.4 jmespath-1.0.0 parsel-1.6.0 protego-0.2.1 pyasn1-0.4.8 pyasn1-modules-0.2.8 queuelib-1.6.2 requests-file-1.5.1 scrapy-2.6.1 service-identity-21.1.0 tldextract-3.3.0 twisted-iocpsupport-1.0.2 urllib3-1.25.11 zope.interface-5.4.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: selenium 4.2.0 has requirement urllib3[secure,socks]~=1.26, but you'll have urllib3 1.25.11 which is incompatible.\n"
     ]
    }
   ],
   "source": [
    "!pip install scrapy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "from scrapy.http import TextResponse\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"http://quotes.toscrape.com/\"\n",
    "r = requests.get(url)\n",
    "response = TextResponse(r.url,body=r.text,encoding=\"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<200 http://quotes.toscrape.com/>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<a href=\"/\" style=\"text-decoration: none\">Quotes to Scrape</a>'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.css(\"a\").extract_first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Albert Einstein',\n",
       " 'J.K. Rowling',\n",
       " 'Albert Einstein',\n",
       " 'Jane Austen',\n",
       " 'Marilyn Monroe',\n",
       " 'Albert Einstein',\n",
       " 'André Gide',\n",
       " 'Thomas A. Edison',\n",
       " 'Eleanor Roosevelt',\n",
       " 'Steve Martin']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.css(\"small::text\").extract()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Albert Einstein',\n",
       " 'J.K. Rowling',\n",
       " 'Albert Einstein',\n",
       " 'Jane Austen',\n",
       " 'Marilyn Monroe',\n",
       " 'Albert Einstein',\n",
       " 'André Gide',\n",
       " 'Thomas A. Edison',\n",
       " 'Eleanor Roosevelt',\n",
       " 'Steve Martin']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#authors-xpath\n",
    "response.xpath(\"//small/text()\").extract()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['change',\n",
       " 'deep-thoughts',\n",
       " 'thinking',\n",
       " 'world',\n",
       " 'abilities',\n",
       " 'choices',\n",
       " 'inspirational',\n",
       " 'life',\n",
       " 'live',\n",
       " 'miracle',\n",
       " 'miracles',\n",
       " 'aliteracy',\n",
       " 'books',\n",
       " 'classic',\n",
       " 'humor',\n",
       " 'be-yourself',\n",
       " 'inspirational',\n",
       " 'adulthood',\n",
       " 'success',\n",
       " 'value',\n",
       " 'life',\n",
       " 'love',\n",
       " 'edison',\n",
       " 'failure',\n",
       " 'inspirational',\n",
       " 'paraphrased',\n",
       " 'misattributed-eleanor-roosevelt',\n",
       " 'humor',\n",
       " 'obvious',\n",
       " 'simile',\n",
       " 'love',\n",
       " 'inspirational',\n",
       " 'life',\n",
       " 'humor',\n",
       " 'books',\n",
       " 'reading',\n",
       " 'friendship',\n",
       " 'friends',\n",
       " 'truth',\n",
       " 'simile']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#tag text css\n",
    "response.css(\"a[class='tag']::text\").extract()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/tag/change/page/1/',\n",
       " '/tag/deep-thoughts/page/1/',\n",
       " '/tag/thinking/page/1/',\n",
       " '/tag/world/page/1/',\n",
       " '/tag/abilities/page/1/',\n",
       " '/tag/choices/page/1/',\n",
       " '/tag/inspirational/page/1/',\n",
       " '/tag/life/page/1/',\n",
       " '/tag/live/page/1/',\n",
       " '/tag/miracle/page/1/',\n",
       " '/tag/miracles/page/1/',\n",
       " '/tag/aliteracy/page/1/',\n",
       " '/tag/books/page/1/',\n",
       " '/tag/classic/page/1/',\n",
       " '/tag/humor/page/1/',\n",
       " '/tag/be-yourself/page/1/',\n",
       " '/tag/inspirational/page/1/',\n",
       " '/tag/adulthood/page/1/',\n",
       " '/tag/success/page/1/',\n",
       " '/tag/value/page/1/',\n",
       " '/tag/life/page/1/',\n",
       " '/tag/love/page/1/',\n",
       " '/tag/edison/page/1/',\n",
       " '/tag/failure/page/1/',\n",
       " '/tag/inspirational/page/1/',\n",
       " '/tag/paraphrased/page/1/',\n",
       " '/tag/misattributed-eleanor-roosevelt/page/1/',\n",
       " '/tag/humor/page/1/',\n",
       " '/tag/obvious/page/1/',\n",
       " '/tag/simile/page/1/',\n",
       " '/tag/love/',\n",
       " '/tag/inspirational/',\n",
       " '/tag/life/',\n",
       " '/tag/humor/',\n",
       " '/tag/books/',\n",
       " '/tag/reading/',\n",
       " '/tag/friendship/',\n",
       " '/tag/friends/',\n",
       " '/tag/truth/',\n",
       " '/tag/simile/']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#tag url css\n",
    "response.css(\"a[class='tag']::attr(href)\").extract()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scrapy\n",
    "\n",
    "class TechcrunchSpider(scrapy.Spider):\n",
    "    #name of the spider\n",
    "    name = 'techcrunch'\n",
    "\n",
    "    #list of allowed domains\n",
    "    allowed_domains = ['techcrunch.com/feed/']\n",
    "\n",
    "    #starting url for scraping\n",
    "    start_urls = ['http://techcrunch.com/feed/']\n",
    "\n",
    "    #setting the location of the output csv file\n",
    "    custom_settings = {\n",
    "        'FEED_URI' : 'tmp/techcrunch.csv'\n",
    "    }\n",
    "\n",
    "    def parse(self, response):\n",
    "        #Remove XML namespaces\n",
    "        response.selector.remove_namespaces()\n",
    "\n",
    "        #Extract article information\n",
    "        titles = response.xpath('//item/title/text()').extract()\n",
    "        authors = response.xpath('//item/creator/text()').extract()\n",
    "        dates = response.xpath('//item/pubDate/text()').extract()\n",
    "        links = response.xpath('//item/link/text()').extract()\n",
    "\n",
    "        for item in zip(titles,authors,dates,links):\n",
    "            scraped_info = {\n",
    "                'title' : item[0],\n",
    "                'author' : item[1],\n",
    "                'publish_date' : item[2],\n",
    "                'link' : item[3]\n",
    "            }\n",
    "\n",
    "            yield scraped_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
